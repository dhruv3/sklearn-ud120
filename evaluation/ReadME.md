# Lesson 15: Evaluation Metrics

Accuracy is always the ideal way to determine how good our algo is working.

**Confusion Matrix:**

<img width="848" alt="screen shot 2018-09-26 at 1 12 21 pm" src="https://user-images.githubusercontent.com/13077629/46096738-f8971300-c18d-11e8-8af6-697059d7805a.png">

`Recall`: True Positive / (True Positive + False Negative). Out of all the items that are truly positive, how many were correctly classified as positive. Or simply, how many positive items were 'recalled' from the dataset.

`Precision`: True Positive / (True Positive + False Positive). Out of all the items labeled as positive, how many truly belong to the positive class.

